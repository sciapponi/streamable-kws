experiment_name: phi_gru64_no_attn

hydra:
  run:
    dir: ./logs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    name: ${experiment_name}


training:
  batch_size: 128
  epochs: 200
  patience: 10  # Number of epochs to wait before stopping
  min_delta: 0.001  # Minimum improvement in validation loss to qualify as improvement

model:
  _target_: models.Improved_Phi_FC
  num_classes: 35
  n_mel_bins: 40
  hidden_dim: 24  # Used in GRU & attention modules

  matchbox:
    input_channels: 40  # Match n_mel_bins
    base_filters: 48  # Sweet spot between 32â€“64
    block_filters: 24  # Keeps model small, but expressive
    dropout_rate: 0.25  # Light dropout for generalization
    use_se: false  # Optional: disable for pure M4 deploy
    expansion_factor: 0.5

    num_blocks: 3
    sub_blocks_per_block: 3

    kernel_sizes: [7, 5, 3, 3, 3, 5, 3, 1]
    dilations:    [1, 2, 4, 8, 4, 2, 1, 1]

    skip_connections:
      enable_block_skips: true
      enable_sub_block_skips: true
      enable_final_skip: true

  # num_layers: 1

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 1e-5

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   mode: min
#   factor: 0.5
#   patience: 5
#   verbose: true

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   T_0: 10           # Number of epochs for the first restart
#   T_mult: 2         # Increase the period after each restart
#   eta_min: 1e-6     # Minimum learning rate
#   verbose: true

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100         # Adjust based on your total number of epochs
  eta_min: 5e-6      # Final learning rate value
  verbose: true



dataset:
  defaults:
  preload: true
  allowed_classes: ["backward", "bed", "bird", "cat", "dog", "down", "eight", "five", "follow", 
                     "forward", "four", "go", "happy", "house", "learn", "left", "marvin", "nine", 
                     "no", "off", "on", "one", "right", "seven", "sheila", "six", "stop", "three", 
                     "tree", "two", "up", "visual", "wow", "yes", "zero"]
  train:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "training"
    augment: false
  val:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "validation"
    augment: false
  test:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "testing"
    augment: false