experiment_name: phi_FCrec_focused_attn_0.5_label_smoothing

hydra:
  run:
    dir: ./logs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    name: ${experiment_name}


training:
  batch_size: 128
  epochs: 100
  patience: 10  # Number of epochs to wait before stopping
  min_delta: 0.001  # Minimum improvement in validation loss to qualify as improvement

model:
  _target_: models.Improved_Phi_FC_Recurrent
  num_classes: 35
  n_mel_bins: 64
  hidden_dim: 32
  num_layers: 1

optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 1e-5

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   mode: min
#   factor: 0.5
#   patience: 5
#   verbose: true

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#   T_0: 10           # Number of epochs for the first restart
#   T_mult: 2         # Increase the period after each restart
#   eta_min: 1e-6     # Minimum learning rate
#   verbose: true

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100         # Adjust based on your total number of epochs
  eta_min: 5e-6      # Final learning rate value
  verbose: true



dataset:
  defaults:
  preload: false
  allowed_classes: ["backward", "bed", "bird", "cat", "dog", "down", "eight", "five", "follow", 
                     "forward", "four", "go", "happy", "house", "learn", "left", "marvin", "nine", 
                     "no", "off", "on", "one", "right", "seven", "sheila", "six", "stop", "three", 
                     "tree", "two", "up", "visual", "wow", "yes", "zero"]
  train:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "training"
    augment: true
  val:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "validation"
    augment: false
  test:
    _target_: datasets.SpeechCommandsDataset
    root_dir: "speech_commands_dataset"
    subset: "testing"
    augment: false